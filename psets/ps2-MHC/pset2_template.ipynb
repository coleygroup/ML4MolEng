{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Problem Set 2 (MHC) <center>\n",
    "\n",
    "<center> 3.C01/3.C51, 7.C01/7.C51, 10.C01/10.C51, 20.C01/20.C51 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name:</b>\n",
    "\n",
    "<b>Kerberos ID:</b>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from sklearn import preprocessing\n",
    "from sklearn.linear_model import LinearRegression, LogisticRegression, Ridge, Lasso\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neural_network import MLPRegressor, MLPClassifier\n",
    "from sklearn.metrics import ConfusionMatrixDisplay\n",
    "from sklearn.utils import shuffle\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib\n",
    "import torch\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "matplotlib.rc('lines', linewidth=3, color='g')\n",
    "matplotlib.rcParams['axes.linewidth'] = 2.0\n",
    "matplotlib.rcParams['axes.linewidth'] = 2.0\n",
    "matplotlib.rcParams[\"xtick.major.size\"] = 6\n",
    "matplotlib.rcParams[\"ytick.major.size\"] = 6\n",
    "matplotlib.rcParams[\"ytick.major.width\"] = 2\n",
    "matplotlib.rcParams[\"xtick.major.width\"] = 2\n",
    "matplotlib.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Download required data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-MHC/data/amino_acid.csv\n",
    "! wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-MHC/data/ba_train.csv\n",
    "! wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-MHC/data/ba_holdout.csv\n",
    "! wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-MHC/data/el_train.csv\n",
    "! wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/main/psets/ps2-MHC/data/amino_acids.npy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Problem 1: Modeling Binding Affinity</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 (10 points) Encoding amino acids into feature vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "ba_df = pd.read_csv(\"ba_train.csv\") # read data \n",
    "amino_acids = np.load('./amino_acids.npy') # Read all amino acids \n",
    "\n",
    "# Your code to featurize amino acids \n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 (5 points) Modeling with ridge regression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "# Generate random data (replace this with your own code)\n",
    "n_samples = 100\n",
    "train_prediction =  np.linspace(0, 50, n_samples) + np.random.randn(n_samples)\n",
    "train_truth =  np.linspace(0, 50, n_samples) + np.random.randn(n_samples)\n",
    "\n",
    "test_prediction = np.linspace(0, 50, n_samples) + 2 * np.random.randn(n_samples)\n",
    "test_truth = np.linspace(0, 50, n_samples) + 2 * np.random.randn(n_samples)\n",
    "\n",
    "# Code snippet to generate scatter plot \n",
    "fig, ax = plt.subplots(nrows=1, ncols=2, figsize=(10, 5))\n",
    "ax[0].scatter(train_prediction, train_truth, label='Train', alpha=0.5)\n",
    "ax[1].scatter(test_prediction, test_truth, label='Test', alpha=0.5, c='orange')\n",
    "\n",
    "ax[0].set_ylabel(\"True Binding Affinity\")\n",
    "ax[0].set_xlabel(\"Predicted Binding Affinity\")\n",
    "ax[1].set_xlabel(\"Predicted Binding Affinity\")\n",
    "ax[0].set_title('Train')\n",
    "ax[1].set_title('Test')\n",
    "fig.suptitle('Ridge Regression')\n",
    "\n",
    "print(\"Ridge Regression training R^2 score: {:.2f}\".format(1.0))\n",
    "print(\"Ridge Regression testing R^2 score: {:.2f}\".format(1.0))\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (6 points) Modeling with a multi-layer perceptron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "print(\"MLP Regression training R^2 score: {:.2f}\".format(1.0))\n",
    "print(\"MLP Regression testing R^2 score: {:.2f}\".format(1.0))\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "What does the input hidden_layers_sizes = (512, 256, 128) mean?"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "########## Answer ############"
   ],
   "metadata": {
    "collapsed": false,
    "ExecuteTime": {
     "end_time": "2024-02-21T02:23:08.465515700Z",
     "start_time": "2024-02-21T02:23:08.448003500Z"
    }
   },
   "execution_count": 1
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1.4 (Graduate - 5 points) Compute model size"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculate the total of number of parameters in your MLP model.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 (5 points) Chemical transferability of one-hot representations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "# Load the holdout dataset which contains amino acids not seen in the training data\n",
    "ba_holdout = pd.read_csv(\"ba_holdout.csv\") \n",
    "\n",
    "\n",
    "print(\"MLP validation R^2 score: {:.2f}\".format(1.0))\n",
    "print(\"Ridge validation R^2 score: {:.2f}\".format(1.0))\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your validation results and briefly explain."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6 (Graduate - 5 points) Featurize amino acids with physical descriptors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "amino_acid_df = pd.read_csv(\"amino_acid.csv\")\n",
    "amino_acid_df = amino_acid_df.set_index('Amino Acids')\n",
    "\n",
    "\n",
    "print(\"Training R^2 score: {:.2f}\".format(1.0) )\n",
    "print(\"Testing R^2 score: {:.2f}\".format(1.0) )\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 (Graduate - 5 points) Chemical transferability of physical descriptors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "# For the holdout set, encode features with the amino acid descriptor \n",
    "\n",
    "print(\"MLP holdout R^2 score: {:.2f}\".format(1.0))\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Briefly comment on your validation performance and explain why."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Problem 2: Hyperparameter Tuning and Modeling Eluted Ligand Data</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (15 points) Optimize neural network architectures with HyperOpt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "from hyperopt import hp\n",
    "from hyperopt import fmin, tpe, space_eval, Trials\n",
    "from sklearn.model_selection import cross_val_score\n",
    "\n",
    "# Stack train and holdout set together\n",
    "\n",
    "\n",
    "def hyperoptoutput2param(best):\n",
    "    \n",
    "    '''Change hyperopt output to dictionary with values '''\n",
    "    \n",
    "    for key in best.keys():\n",
    "        if key in hyper_dict.keys():\n",
    "            best[key] = hyper_dict[key][ best[key] ] \n",
    "            \n",
    "    return best\n",
    "\n",
    "# Define a dictionary for each parameter range \n",
    "\n",
    "\n",
    "hyper_dict = {\n",
    "    \"hidden_layers\": [(512, 256, 128), (256, 128, 64), (512, 512, 256)],\n",
    "    \"optimizer\": ['sgd', 'adam'],\n",
    "    \"activation\": ['relu', 'tanh'],\n",
    "    \"alpha\":[0.04, 0.08, 0.16]\n",
    "}\n",
    "\n",
    "parameter_space =  { \"hidden_layers\": hp.choice(\"hidden_layers\", hyper_dict['hidden_layers']),\n",
    "            \"optimizer\": hp.choice(\"optimizer\", hyper_dict['optimizer']), \n",
    "            \"activation\": hp.choice(\"activation\", hyper_dict['activation']), \n",
    "            \"alpha\": hp.choice(\"alpha\", hyper_dict['alpha'])\n",
    "                    }\n",
    "\n",
    "# Evaluation function \n",
    "# args should be a dict, with keys for number of hidden layers, optimizer, activation, and alpha\n",
    "def model_eval(args):\n",
    "\n",
    "    '''Take suggested arguments and perform model evaluation'''\n",
    "\n",
    "    # your code here to train MLPRegressors and to run CV score on the training data \n",
    "\n",
    "    cv_score = 1.0\n",
    "    # return the negative of the CV score to ensure we maximize R^2 by minimizing the loss\n",
    "    return -cv_score\n",
    "\n",
    "\n",
    "print(\"Start trials\") \n",
    "\n",
    "trials = Trials()\n",
    "best = fmin(model_eval, parameter_space, algo=tpe.suggest, max_evals=40, trials=trials) # this will take a while to run \n",
    "best = hyperoptoutput2param(best)\n",
    "\n",
    "print(\"Best parameter set: {}\".format(best))\n",
    "print(\"Best loss from CV {:.2f}\".format(-trials.best_trial['result']['loss']))\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a MLP with the hyperparameters obtained."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your parameter set. Have your predictions improved? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (Graduate - 5 points) Applying MLPs to classifying eluted ligands"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "el_df = pd.read_csv(\"el_train.csv\") # read data \n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center> Problem 3: Accelerating Neural Networks with GPUs </center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 (2 points) Request a GPU on Google Colab"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell as-is, with no modifications, to show that you successfully requested a GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if your GPU is requested successfully or not \n",
    "assert torch.cuda.device_count() != 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (12 points) Build Datasets and DataLoaders in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate a validation set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A SequenceDataset class for you to construct your dataloaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Generate dataset\n",
    "class SequenceDataset(Dataset):\n",
    "    def __init__(self, X, y):\n",
    "        self.X = torch.Tensor(np.array(X))  # store X as a pytorch Tensor\n",
    "        self.y = torch.Tensor(np.array(y))  # store y as a pytorch Tensor\n",
    "        self.len=len(self.X)                # number of samples in the data \n",
    "\n",
    "    def __getitem__(self, index):\n",
    "        return self.X[index], self.y[index] # get the appropriate item\n",
    "\n",
    "    def __len__(self):\n",
    "        return self.len"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill in the skeleton code to construct your Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "train_data = SequenceDataset(# fill in arguments\n",
    "val_data = SequenceDataset(# fill in arguments\n",
    "test_data = SequenceDataset(# fill in arguments\n",
    "\n",
    "batch_size = # fill in batch size\n",
    "train_dataloader = DataLoader(train_data, batch_size=batch_size, shuffle=# fill in shuffle argument\n",
    "val_dataloader = DataLoader(val_data, batch_size=batch_size, shuffle=# fill in shuffle argument\n",
    "test_dataloader = DataLoader(test_data, batch_size=batch_size, shuffle=# fill in shuffle argument\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run this cell to check that your DataLoaders work as expected."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "for loader in [train_dataloader, val_dataloader, test_dataloader]:\n",
    "    for index, batch in enumerate(loader): \n",
    "        # Your batch returns a X, y stacked in a batch \n",
    "        X_batch, y_batch = batch[0], batch[1]\n",
    "        if index == 0:\n",
    "            print(X_batch.shape, y_batch.shape)\n",
    "        \n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.3 (15 points) Define the MLP in PyTorch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code snippet to understand how the linear layer works in PyTorch. Take careful note of the dimensions of the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "linear = torch.nn.Linear(2, 3)\n",
    "\n",
    "input_tensor = torch.ones((4, 2))\n",
    "output_tensor = linear(input_tensor)\n",
    "\n",
    "print(input_tensor, output_tensor, input_tensor.shape, output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code snippet to understand how the ReLU layer works in PyTorch (the Tanh layer is similar). Take careful note of the dimensions of the input and output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "relu = torch.nn.ReLU()\n",
    "\n",
    "input_tensor = torch.ones((4, 2))\n",
    "output_tensor = relu(input_tensor)\n",
    "\n",
    "print(input_tensor, output_tensor, input_tensor.shape, output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Look at the following code snippet to understand how to stack layers with the Sequential module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "layer1 = torch.nn.Linear(2, 3)\n",
    "layer2 = torch.nn.Linear(3, 4)\n",
    "\n",
    "sequential = torch.nn.Sequential(layer1, layer2)\n",
    "\n",
    "input_tensor = torch.ones((5, 2))\n",
    "output_tensor = sequential(input_tensor)\n",
    "\n",
    "print(input_tensor, output_tensor, input_tensor.shape, output_tensor.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Build your MLP within the following torch.nn.Module object. Remember to use your hyperparameters from part 2.1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "class SequenceMLP(torch.nn.Module):\n",
    "    def __init__(self):\n",
    "        # You can modify this method to pass hyperparameters above, but this is not necessary\n",
    "        # since we already have fixed hyperparameters\n",
    "        super().__init__()\n",
    "        \n",
    "        # Implement your code here\n",
    "        \n",
    "        self.model = # fill in model here\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.model(x)\n",
    "        \n",
    "        return x\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.4 (10 points) Implement functions for training and testing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Define your model, device, and optimizer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "# device to train on\n",
    "device = 'cuda:0'\n",
    "# define your model\n",
    "model = SequenceMLP().to(device)\n",
    "\n",
    "# define your optimizer\n",
    "optimizer = # fill in optimizer here\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Implement your training and validation loops here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "def train(model, dataloader, optimizer, device):\n",
    "    \n",
    "    '''\n",
    "    A function train on the entire dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module): your model from before \n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader object for the train data\n",
    "        optimizer (torch.optim.Optimizer(()): optimizer object to interface gradient calculation and optimization \n",
    "        device (str): Your device (usually 'cuda:0' for your GPU)\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "\n",
    "    epoch_loss = []\n",
    "    model.train() # Set model to training mode \n",
    "    \n",
    "    for batch in dataloader:    \n",
    "        X, y = batch\n",
    "        X = X.to(device)\n",
    "        y = y.to(device)\n",
    "        \n",
    "        # train your model on each batch here \n",
    "        y_pred = model(X)\n",
    "        \n",
    "        loss = # fill in loss here\n",
    "        epoch_loss.append(loss.item())\n",
    "        \n",
    "        # run backpropagation given the loss you defined\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "    return np.array(epoch_loss).mean()\n",
    "\n",
    "\n",
    "def validate(model, dataloader, device):\n",
    "    \n",
    "    '''\n",
    "    A function validate on the validation dataset for one epoch .\n",
    "    \n",
    "    Args: \n",
    "        model (torch.nn.Module): your model for before \n",
    "        dataloader (torch.utils.data.DataLoader): DataLoader object for the validation data\n",
    "        device (str): Your device (usually 'cuda:0' for your GPU)\n",
    "        \n",
    "    Returns: \n",
    "        float: loss averaged over all the batches \n",
    "    \n",
    "    '''\n",
    "    \n",
    "    val_loss = []\n",
    "    model.eval() # Set model to evaluation mode \n",
    "    with torch.no_grad():    \n",
    "        for batch in dataloader:\n",
    "            X, y = batch\n",
    "            X = X.to(device)\n",
    "            y = y.to(device)\n",
    "            \n",
    "            # validate your model on each batch here \n",
    "            y_pred = model(X)\n",
    "\n",
    "            loss = # fill in loss here\n",
    "            val_loss.append(loss.item())\n",
    "            \n",
    "    return np.array(val_loss).mean()\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and validate your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"epoch\", \"train loss\", \"validation loss\")\n",
    "\n",
    "val_loss_curve = []\n",
    "train_loss_curve = []\n",
    "\n",
    "for epoch in range(400):\n",
    "    \n",
    "    # Compute train your model on training data\n",
    "    epoch_loss = train(model, train_dataloader, optimizer,  device=device)\n",
    "    \n",
    "    # Validate your on validation data \n",
    "    val_loss = validate(model, val_dataloader, device=device) \n",
    "    \n",
    "    # Record train and loss performance \n",
    "    train_loss_curve.append(epoch_loss)\n",
    "    val_loss_curve.append(val_loss)\n",
    "    \n",
    "    print(epoch, epoch_loss, val_loss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot your train and validation loss curves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(val_loss_curve)\n",
    "plt.plot(train_loss_curve)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Compute your R^2 on the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "from sklearn.metrics import r2_score\n",
    "\n",
    "# code to compute r^2 goes here\n",
    "model.eval()\n",
    "########### Code #############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
