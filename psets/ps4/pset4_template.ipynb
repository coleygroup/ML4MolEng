{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KF1v5UP-C5QN"
   },
   "source": [
    "#  <center> Problem Set 4 <center>\n",
    "\n",
    "<center> 3.C01/3.C51, 7.C01/7.C51, 10.C01/10.C51, 20.C01/20.C51 <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Zq3aSfRfC5QO"
   },
   "outputs": [],
   "source": [
    "import os \n",
    "import glob \n",
    "import PIL\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from PIL import Image\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import confusion_matrix\n",
    "from tqdm import tqdm\n",
    "import imageio\n",
    "from skimage import io, color\n",
    "\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "from torchvision.io import read_image\n",
    "import torchvision.transforms as transforms\n",
    "from torch import nn \n",
    "from torchvision.models import vgg16\n",
    "import torch.optim as optim"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "wqMAHB7TC5QO"
   },
   "source": [
    "## Part 1: Classifying  Steel  Surface  Defects"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "B4h4WKFqC5QP",
    "outputId": "c18003d4-4c94-429c-b913-885b175912df"
   },
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/vikram-sundar/ML4MolEng_Spring2022/master/psets/ps5/data/neu_surface_defect_jpg.tar.gz\n",
    "!tar -xf neu_surface_defect_jpg.tar.gz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "YzfZx-Q8C5QP"
   },
   "source": [
    "### 1.1 (15 points)  Build Image Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "faPfoTY1tIQk"
   },
   "source": [
    "Get all the image filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UgjAC58yC5QP"
   },
   "outputs": [],
   "source": [
    "files = glob.glob(os.path.join('neu_surfae_defect_images', \"*.jpg\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qTxZFkrXtP4V"
   },
   "source": [
    "Visualize a random image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "UQsEng4FC5QQ",
    "outputId": "2a932eb0-80e2-4f08-d7b9-e15ad7b67f20"
   },
   "outputs": [],
   "source": [
    "idx = 30\n",
    "img = Image.open(files[idx])\n",
    "print(files[idx])\n",
    "img"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PC1P-yi4ts3e"
   },
   "source": [
    "Your ImageDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eFyfSljJC5QQ"
   },
   "outputs": [],
   "source": [
    "# dictionary labels \n",
    "label_dict = {\n",
    "'Cr': 0, \n",
    "'In': 1, \n",
    "'Pa': 2,\n",
    "'PS': 3, \n",
    "'RS': 4,\n",
    "'Sc': 5\n",
    "}\n",
    "\n",
    "class ImageDataset(Dataset):\n",
    "    def __init__(self, image_paths, transform=None):\n",
    "        \n",
    "        '''\n",
    "        Image dataset object that loads and transforms images. \n",
    "        \n",
    "        '''\n",
    "        \n",
    "        self.paths = image_paths\n",
    "        self.transform = transform\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.paths)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        #########your implementation here######### \n",
    "        \n",
    "        \n",
    "        # read images given file path \n",
    "        img = # fill in\n",
    "        label = # fill in\n",
    "        #########your implementation here#########\n",
    "\n",
    "        # transform images \n",
    "        if self.transform:\n",
    "            img = self.transform(img)\n",
    "\n",
    "        sample = img, label\n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FH9_Oe1pC5QQ"
   },
   "source": [
    "Transform your dataset, split the data, and define your Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "h3urcd98C5QR"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Zgt2ZZpGC5QR"
   },
   "source": [
    "### 1.2 (10 points) Understand the Model Architecture"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Avv-tdKwvm5Z"
   },
   "source": [
    "Define and load a pretrained VGG16 model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Z9D3Hn4LC5QR"
   },
   "outputs": [],
   "source": [
    "class VGG_fc1(nn.Module):\n",
    "    def __init__(self, pretrain=True):\n",
    "        super(VGG_fc1, self).__init__()\n",
    "        self.features = vgg16(pretrained=pretrain).features # convolutional layers\n",
    "        self.avgpool = vgg16(pretrained=pretrain).avgpool\n",
    "        self.fc1 = vgg16(pretrained=pretrain).classifier[0] # first layer of classifier\n",
    "        \n",
    "    def forward(self, x):\n",
    "        \"\"\"Extract first fully connected feature vector\"\"\"\n",
    "        # Apply convolutions\n",
    "        x = self.features(x)\n",
    "        # Apply pooling\n",
    "        x = self.avgpool(x)\n",
    "        # Flatten and convert to vectors\n",
    "        x = torch.flatten(x, 1)\n",
    "        x = self.fc1(x)\n",
    "        return x\n",
    "    \n",
    "model = VGG_fc1(pretrain=True).eval() # turn model into evaluation mode"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jh--PHe9v044"
   },
   "source": [
    "The model architecture."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Sh4LR7tnC5QR",
    "outputId": "57681b6a-5f2f-4b8a-e5db-50f52c31a703"
   },
   "outputs": [],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QbXs5RnnC5QS"
   },
   "source": [
    "Choose an image from your training set and visualize 4 channels in each of layers 1, 5, and 10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-aoYSiepC5QS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kxYN36OYwrN2"
   },
   "source": [
    "What do you observe?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "49MbcpLrwsv6"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AlKzUBdQC5QS"
   },
   "source": [
    "### 1.3 (20 points) Train a Classifier with Transfer Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "q6QTstsoC5QS"
   },
   "source": [
    "Define a VGG-based transfer learning classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0tSdzWsIC5QS"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "AODiLlZaC5QS"
   },
   "source": [
    "Train your classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nD08S_rJC5QT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "xwEKhXtWC5QT"
   },
   "source": [
    "Compute and plot a test confusion matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "uRYBP223C5QT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "SAceJrwMC5QT"
   },
   "source": [
    "Why do you need to resize images to specific shapes and normalize pixel values to specific values for each color channel?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "GiZpNjYRC5QT"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "VOtoLwY9C5QT"
   },
   "source": [
    "What are the benefits of transfer learning versus training the entire stack (CNN + MLP) again. What are the potential limitations of this approach? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hw0coHVIC5QU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "5l5Nu4alC5QU"
   },
   "source": [
    "### 1.4 (15 points) Obtain Saliency Maps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "kBDTnUCfC5QU"
   },
   "source": [
    "Compute the saliency map for two images of each class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "6Iq2XV7ZC5QU"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "jZLc2oDl1for"
   },
   "source": [
    "Comment on any pattern you observe in the saliency maps."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "-9iAFX702XJW"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cFIapfFa-ux2"
   },
   "source": [
    "# Part 2: Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Pm2OgIu-3SG0"
   },
   "source": [
    "### 2.1 (15 points) Build Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "dvZpB38y3rLl"
   },
   "source": [
    "Download and unzip data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "HoqvPlv1-ux4",
    "outputId": "4a80997d-49cf-4582-847d-f5f26123e301"
   },
   "outputs": [],
   "source": [
    "!wget -O part2data.zip https://raw.githubusercontent.com/vikram-sundar/ML4MolEng_Spring2022/master/psets/ps5/data/part2_data.zip\n",
    "! rm -r segmentation_data\n",
    "! mkdir segmentation_data/\n",
    "! unzip part2data.zip -d segmentation_data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9HpvZaPk3njm"
   },
   "source": [
    "Parse data from image filepaths."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "NfemAdUY-ux6"
   },
   "outputs": [],
   "source": [
    "paths = glob.glob(\"segmentation_data/part2_segdata/*\")\n",
    "\n",
    "def parse_data(path):\n",
    "\n",
    "    # define data folders\n",
    "    x_path = os.path.join(path, \"images/\")\n",
    "    y_path = os.path.join(path, \"masks/\")\n",
    "\n",
    "    # get all data paths \n",
    "    x_file = glob.glob(os.path.join(x_path, \"*.png\"))[0]\n",
    "    y_files = glob.glob(os.path.join(y_path, \"*.png\"))\n",
    "\n",
    "    # parse in data\n",
    "    x = imageio.imread(x_file)\n",
    "    \n",
    "    masks = np.array([imageio.imread(y) for y in y_files])\n",
    "    y = np.zeros_like(masks[0])\n",
    "    for y_raw in masks:\n",
    "        y = np.maximum(y, y_raw)\n",
    "\n",
    "    # normalize\n",
    "    x = x / 255.0\n",
    "    y = y / 255.0\n",
    "\n",
    "    return x, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aWxWYl2256rf"
   },
   "source": [
    "Load one image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "hjyk1BTM-ux6",
    "outputId": "773b21e8-f422-4edd-b9c7-f75f59a8fc0d"
   },
   "outputs": [],
   "source": [
    "x, y = parse_data(paths[0])\n",
    "print(x.shape, y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "F-wPI44I-ux7"
   },
   "source": [
    "Your ImageDataset class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "lM1pTVbN-ux7"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "aQrzraKm7iq8"
   },
   "source": [
    "Split your data and load your DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ak8iyav67jk0"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "9viPOISW-ux7"
   },
   "source": [
    "Is it necessary to apply random translation to your image?  Briefly justify your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "eoCRG6vf-ux8"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "fDr6-J9k-ux8"
   },
   "source": [
    "### 2.2 (20 points) Train a U-Net Model that Performs Image Segmentation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "KOIzGL568NOC"
   },
   "source": [
    "Implement Dice loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "rdQWQX4T8PFZ"
   },
   "outputs": [],
   "source": [
    "def dice_loss(pred, target):\n",
    "    \"\"\"Calculate Dice loss.\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "        pred:\n",
    "            predictions from the model\n",
    "        target:\n",
    "            ground truth label\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "c1jkSXv68te1"
   },
   "source": [
    "The U-Net Model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Gid7rspA-ux8"
   },
   "outputs": [],
   "source": [
    "class DownSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, name=None):\n",
    "        super(DownSampling, self).__init__()\n",
    "\n",
    "        self.conv = ConvBlock(in_channels, out_channels, kernel_size)\n",
    "        self.max_pool = nn.MaxPool2d(kernel_size=2, stride=2)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        conv_out = self.conv(x)\n",
    "        output = self.max_pool(conv_out)\n",
    "\n",
    "        return output, conv_out\n",
    "\n",
    "\n",
    "class UpSampling(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, name=None):\n",
    "        super(UpSampling, self).__init__()\n",
    "\n",
    "        self.conv = ConvBlock(in_channels, out_channels, kernel_size)\n",
    "        self.conv_t = nn.ConvTranspose2d(out_channels, out_channels, kernel_size, \\\n",
    "                                         padding=1, stride=2, output_padding=1)\n",
    "\n",
    "\n",
    "    def forward(self, x, skip):\n",
    "\n",
    "        conv_out = self.conv(x)\n",
    "        output = self.conv_t(conv_out)\n",
    "\n",
    "        output += skip\n",
    "\n",
    "        return output\n",
    "\n",
    "\n",
    "class ConvBlock(nn.Module):\n",
    "\n",
    "    def __init__(self, in_channels, out_channels, kernel_size, padding=1, stride=1, name=None):\n",
    "        super(ConvBlock, self).__init__()\n",
    "\n",
    "        block = []\n",
    "        # first conv layer\n",
    "        block.append(nn.Conv2d(in_channels, out_channels, kernel_size, \\\n",
    "                               padding=padding, stride=stride))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # second conv layer\n",
    "        block.append(nn.Conv2d(out_channels, out_channels, kernel_size, \\\n",
    "                               padding=padding, stride=stride))\n",
    "        block.append(nn.ReLU())\n",
    "        block.append(nn.BatchNorm2d(out_channels))\n",
    "\n",
    "        # make sequential\n",
    "        self.conv_block = nn.Sequential(*block)\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        output = self.conv_block(x)\n",
    "\n",
    "        return output\n",
    "    \n",
    "    \n",
    "class UNet(nn.Module):\n",
    "\n",
    "    def __init__(self, num_kernel=8, kernel_size=3, dim=4, target_dim=1):\n",
    "        \"\"\"UNet\n",
    "\n",
    "        Inputs:\n",
    "            num_kernel: int\n",
    "                number of kernels to use for the first layer\n",
    "            kernel_size: int\n",
    "                size of the kernel for the first layer\n",
    "            dims: int\n",
    "                number of color channels for input images \n",
    "            target_dim: int \n",
    "                number of channels for the output mask\n",
    "        \"\"\"\n",
    "\n",
    "        super(UNet, self).__init__()\n",
    "\n",
    "        self.num_kernel = num_kernel\n",
    "        self.kernel_size = kernel_size\n",
    "        self.dim = dim\n",
    "        self.target_dim = 1\n",
    "\n",
    "        # encode\n",
    "        self.encode_1 = DownSampling(self.dim, num_kernel, kernel_size)\n",
    "        self.encode_2 = DownSampling(num_kernel, num_kernel*2, kernel_size)\n",
    "        self.encode_3 = DownSampling(num_kernel*2, num_kernel*4, kernel_size)\n",
    "        self.encode_4 = DownSampling(num_kernel*4, num_kernel*8, kernel_size)\n",
    "\n",
    "        # bridge\n",
    "        self.bridge = nn.Conv2d(num_kernel*8, num_kernel*16, kernel_size, padding=1, stride=1)\n",
    "\n",
    "        # decode\n",
    "        self.decode_4 = UpSampling(num_kernel*16, num_kernel*8, kernel_size)\n",
    "        self.decode_3 = UpSampling(num_kernel*8, num_kernel*4, kernel_size)\n",
    "        self.decode_2 = UpSampling(num_kernel*4, num_kernel*2, kernel_size)\n",
    "        self.decode_1 = UpSampling(num_kernel*2, num_kernel, kernel_size)\n",
    "\n",
    "        self.segment = nn.Conv2d(num_kernel, self.target_dim, 1, padding=0, stride=1)\n",
    "        self.activate = nn.Sigmoid()\n",
    "\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        x, skip_1 = self.encode_1(x)\n",
    "        x, skip_2 = self.encode_2(x)\n",
    "        x, skip_3 = self.encode_3(x)\n",
    "        x, skip_4 = self.encode_4(x)\n",
    "\n",
    "        x = self.bridge(x)\n",
    "\n",
    "        x = self.decode_4(x, skip_4)\n",
    "        x = self.decode_3(x, skip_3)\n",
    "        x = self.decode_2(x, skip_2)\n",
    "        x = self.decode_1(x, skip_1)\n",
    "\n",
    "        x = self.segment(x)\n",
    "\n",
    "        pred = self.activate(x)\n",
    "\n",
    "        return pred\n",
    "\n",
    "\n",
    "    def args_dict(self):\n",
    "        \"\"\"model arguments to be saved\n",
    "        \"\"\"\n",
    "\n",
    "        model_args = {'dim': self.dim,\n",
    "                      'target_dim': self.target_dim,\n",
    "                      'num_kernel' : self.num_kernel,\n",
    "                      'kernel_size' : self.kernel_size}\n",
    "\n",
    "        return model_args\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "_zDf1opf9TK_"
   },
   "source": [
    "Example model usage."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "H7iCvQie-ux9",
    "outputId": "41a09275-74aa-4ea8-ce6b-b7c987e895bf"
   },
   "outputs": [],
   "source": [
    "model = UNet()\n",
    "y = model(torch.randn(1, 4, 256, 256))\n",
    "\n",
    "print(y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "GLG1gczo9YbG"
   },
   "source": [
    "A function to plot a segmentation map."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "llCt40io9Yuu"
   },
   "outputs": [],
   "source": [
    "def plot_seg(image, pred_seg, true_seg, mask_cutoff=0.1):\n",
    "\n",
    "    \"\"\" Visualize segmentation results.\n",
    "    Inputs:\n",
    "        image: orginal image, shape: 256 x 256 x 4\n",
    "        pred_seg: predicted mask, shape: 256 x 256 x 1 or 256 x 256 \n",
    "        true_seg: true mask, shape: 256 x 256 x 1 or 256 x 256\n",
    "        mask_cutoff: if the mask values is larger than mask_cutoff, the mask will appear on the image\n",
    "    \"\"\"\n",
    "    \n",
    "    fig, ax = plt.subplots(1, 3, sharex='col', sharey='row')\n",
    "    fig.set_size_inches((15,15))\n",
    "    \n",
    "    pred_seg = pred_seg.squeeze()\n",
    "    true_seg = true_seg.squeeze()\n",
    "\n",
    "    ax[0].set_title(\"Original Image\")\n",
    "    ax[1].set_title(\"Prediction\")\n",
    "    ax[2].set_title(\"Ground Truth\")\n",
    "\n",
    "    pred_mask = np.zeros((256, 256))\n",
    "    pred_mask[pred_seg > mask_cutoff] = 1\n",
    "    y_mask = np.zeros((256, 256))\n",
    "    y_mask[true_seg > mask_cutoff] = 1\n",
    "\n",
    "    ax[0].imshow(image)\n",
    "    ax[1].imshow(color.label2rgb(pred_mask,image,colors=[(255,0,0)],alpha=0.0025, bg_label=0, bg_color=None))\n",
    "    ax[2].imshow(color.label2rgb(y_mask,image,colors=[(255,0,0)],alpha=0.0025, bg_label=0, bg_color=None))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "2XscmSYt-uyC"
   },
   "source": [
    "Train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "kQ90L_IM-uyC"
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "CPRRoHwu-uyC"
   },
   "source": [
    "Show segmentation results for 3 images from the test dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jB6BgqPs-uyD"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "collapsed_sections": [
    "AlKzUBdQC5QS",
    "5l5Nu4alC5QU",
    "fDr6-J9k-ux8"
   ],
   "name": "pset5_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
