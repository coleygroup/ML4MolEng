{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "wc_fzIDFu27D"
   },
   "source": [
    "#  <center> Problem Set 5 <center>\n",
    "\n",
    "<center> 3.C01/3.C51, 7.C01/7.C51, 10.C01/10.C51, 20.C01/20.C51 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "D86KaQiJu27R"
   },
   "source": [
    "## Part 1: Predicting molecular properties with Graph Convolutional Nets "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "J9egPqYGu27S"
   },
   "source": [
    "### 1.1 (5 points) Install and try out RDkit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hUVPzIPKvxPf"
   },
   "source": [
    "This is a hack to install RDKit, without needing to install conda (which might take minutes). If you have anaconda installed, you can install RDKit from anaconda."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "yaqogUbau27S",
    "outputId": "72230ac4-73d3-43c2-cfb6-f4d8a5e95bb9"
   },
   "outputs": [],
   "source": [
    "url = 'https://anaconda.org/rdkit/rdkit/2018.09.1.0/download/linux-64/rdkit-2018.09.1.0-py36h71b666b_1.tar.bz2'\n",
    "!curl -L $url | tar xj lib\n",
    "!mv lib/python3.6/site-packages/rdkit /usr/local/lib/python3.7/dist-packages/\n",
    "\n",
    "x86 = '/usr/lib/x86_64-linux-gnu'\n",
    "!mv lib/*.so.* $x86/\n",
    "!ln -s $x86/libboost_python3-py36.so.1.65.1 $x86/libboost_python3.so.1.65.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Kh0SNwnEu27T"
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from rdkit import Chem, DataStructs\n",
    "from rdkit.Chem import Descriptors,Crippen\n",
    "from rdkit.Chem import Draw\n",
    "from rdkit.Chem.Draw import IPythonConsole\n",
    "import pandas as pd\n",
    "import sys\n",
    "import torch \n",
    "from tqdm import tqdm\n",
    "import itertools\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from rdkit import RDLogger   \n",
    "RDLogger.DisableLog('rdApp.*') # turn off RDKit warning message "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "XUQm_AT7E5GQ"
   },
   "source": [
    "Optional: mount your Google Drive to save your model and files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "jrooBdScu27T"
   },
   "outputs": [],
   "source": [
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "mydrive = '/content/drive/MyDrive'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8pKZcsSIE5GR"
   },
   "source": [
    "Example: make a Mol object."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qgZjTmmeu27T"
   },
   "outputs": [],
   "source": [
    "dopamine_mol = Chem.MolFromSmiles(\"C1=CC(=C(C=C1CCN)O)O\") # Dopamine \n",
    "caffeine_mol = Chem.MolFromSmiles(\"CN1C=NC2=C1C(=O)N(C(=O)N2C)C\") # Caffeine "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "P5G2CzxgE5GS"
   },
   "source": [
    "Arrange molecules in a grid image."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "4__JJvjju27U"
   },
   "outputs": [],
   "source": [
    "# Arrange molecules in a grid image\n",
    "Draw.MolsToGridImage([dopamine_mol, caffeine_mol])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "p-pHcEc5dzJ2"
   },
   "source": [
    "Use RDKit to visualize molecule line drawings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "As-wlFiOduUt"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "6vuVsOhYu27U"
   },
   "source": [
    "### 1.2 (10 points) Construct Molecular Graph Datasets and DataLoaders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ADhhYCE6dVsF"
   },
   "outputs": [],
   "source": [
    "! wget https://raw.githubusercontent.com/vikram-sundar/ML4MolEng_Spring2022/master/psets/ps4/data/qm9.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "cngsLJBKE5GV"
   },
   "source": [
    "A SMILES to graph conversion function."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "nq0h_UJxu27V"
   },
   "outputs": [],
   "source": [
    "def smiles2graph(smiles):\n",
    "    '''\n",
    "    Transform smiles into a list of atomic numbers and an edge array\n",
    "    \n",
    "    Args: \n",
    "        smiles (str): SMILES strings\n",
    "    \n",
    "    Returns: \n",
    "        z(np.array), A (np.array): list of atomic numbers, edge array\n",
    "    '''\n",
    "    \n",
    "    mol = Chem.MolFromSmiles( smiles ) # no hydrogen \n",
    "    z = np.array( [atom.GetAtomicNum() for atom in mol.GetAtoms()] )\n",
    "    A = np.stack(Chem.GetAdjacencyMatrix(mol)).nonzero()\n",
    "    \n",
    "    return z, A"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7oqI1J2FE5GW"
   },
   "source": [
    "Read in the DataFrame, shuffle its rows, and store its properties as lists."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "x9TuCR_YE5GW"
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from sklearn.utils import shuffle\n",
    "\n",
    "df = pd.read_csv(\"qm9.csv\")\n",
    "df = shuffle(df).reset_index()\n",
    "\n",
    "################ Code #################\n",
    "\n",
    "AtomicNum_list = []\n",
    "Edge_list = []\n",
    "y_list = []\n",
    "Natom_list = []\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "QcfpzNv8E5GX"
   },
   "source": [
    "A GraphDataset class for you to store graphs in PyTorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "YVSxuShzu27W"
   },
   "outputs": [],
   "source": [
    "class GraphDataset(torch.utils.data.Dataset):\n",
    "    def __init__(self,\n",
    "                 AtomicNum_list, \n",
    "                 Edge_list, \n",
    "                 Natom_list, \n",
    "                 y_list):\n",
    "        \n",
    "        '''\n",
    "        GraphDataset object\n",
    "        \n",
    "        Args: \n",
    "            z_list (list of torch.LongTensor)\n",
    "            a_list (list of torch.LongTensor)\n",
    "            N_list (list of int)\n",
    "            y_list (list of torch.FloatTensor)\n",
    "\n",
    "        '''\n",
    "        self.AtomicNum_list = AtomicNum_list # atomic number\n",
    "        self.Edge_list = Edge_list # edge list \n",
    "        self.Natom_list = Natom_list # Number of atoms \n",
    "        self.y_list = y_list # properties to predict \n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.Natom_list)\n",
    "\n",
    "    def __getitem__(self, idx):\n",
    "        \n",
    "        AtomicNum = torch.LongTensor(self.AtomicNum_list[idx])\n",
    "        Edge = torch.LongTensor(self.Edge_list[idx])\n",
    "        Natom = self.Natom_list[idx]\n",
    "        y = torch.Tensor(self.y_list[idx])\n",
    "        \n",
    "        return AtomicNum, Edge, Natom, y"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "EIG6fURIE5Gb"
   },
   "source": [
    "Split your dataset into train, validation, and test and define the GraphDataset class for each."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "1pSpFsj-E5Gc"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "from sklearn.model_selection import train_test_split\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "train_dataset = # fill this in\n",
    "val_dataset = # fill this in\n",
    "test_dataset = # fill this in"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "NyJh30MBE5Ge"
   },
   "source": [
    "A graph collation function to batch multiple graphs into one batch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "0lPCLjWpu27X"
   },
   "outputs": [],
   "source": [
    "def collate_graphs(batch):\n",
    "    '''Batch multiple graphs into one batched graph\n",
    "    \n",
    "    Args:\n",
    "    \n",
    "        batch (tuple): tuples of AtomicNum, Edge, Natom and y obtained from GraphDataset.__getitem__() \n",
    "        \n",
    "    Return \n",
    "        (tuple): Batched AtomicNum, Edge, Natom, y\n",
    "    \n",
    "    '''\n",
    "    \n",
    "    AtomicNum_batch = []\n",
    "    Edge_batch = []\n",
    "    Natom_batch = []\n",
    "    y_batch = []\n",
    "\n",
    "    cumulative_atoms = np.cumsum([0] + [b[2] for b in batch])[:-1]\n",
    "    \n",
    "    for i in range(len(batch)):\n",
    "        z, a, N, y = batch[i]\n",
    "        index_shift = cumulative_atoms[i]\n",
    "        a = a + index_shift\n",
    "        AtomicNum_batch.append(z) \n",
    "        Edge_batch.append(a)\n",
    "        Natom_batch.append(N)\n",
    "        y_batch.append(y)\n",
    "        \n",
    "    AtomicNum_batch = torch.cat(AtomicNum_batch)\n",
    "    Edge_batch = torch.cat(Edge_batch, dim=1)\n",
    "    Natom_batch = Natom_batch\n",
    "    y_batch = torch.cat(y_batch)\n",
    "    \n",
    "    return AtomicNum_batch, Edge_batch, Natom_batch, y_batch "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "TZu1f0hME5Gh"
   },
   "source": [
    "An example use of collate_graph."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "sDVilSwqdVsI"
   },
   "outputs": [],
   "source": [
    "# Define graph 1 \n",
    "AtomicNum1 = torch.LongTensor([6, 6, 7])\n",
    "Edge1 = torch.LongTensor([[0, 2, 2, 1], \n",
    "                       [2, 0, 1, 2]])\n",
    "Natom1 = 3\n",
    "y1 =  torch.Tensor([74.18])\n",
    "\n",
    "# Define graph 2 \n",
    "AtomicNum2 = torch.LongTensor([6, 6, 8])\n",
    "Edge2 = torch.LongTensor([[0, 2, 2, 1], \n",
    "                       [2, 0, 1, 2]])\n",
    "Natom2 = 3\n",
    "y2 = torch.Tensor([64.32])\n",
    "\n",
    "graph1 = (AtomicNum1, Edge1, Natom1, y1)\n",
    "graph2 = (AtomicNum2, Edge2, Natom2, y2)\n",
    "\n",
    "collate_graphs((graph1, graph2))  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "3f3sGR1XdVsJ"
   },
   "source": [
    "Defining the train and test DataLoaders with the above functions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "QplWbt60dVsJ"
   },
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "train_loader = DataLoader(train_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)\n",
    "\n",
    "val_loader = DataLoader(val_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)\n",
    "\n",
    "test_loader = DataLoader(test_dataset,\n",
    "                          batch_size=512, \n",
    "                          collate_fn=collate_graphs,shuffle=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "pND0eELru27Y"
   },
   "source": [
    "### 1.3 (20 points) Complete the definition of a GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "DB1bdNJpE5Gm"
   },
   "source": [
    "The scatter_add function for use in your node updates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "07W6nUPkdVsK"
   },
   "outputs": [],
   "source": [
    "from itertools import repeat\n",
    "def scatter_add(src, index, dim_size, dim=-1, fill_value=0):\n",
    "    \n",
    "    '''\n",
    "    Sums all values from the src tensor into out at the indices specified in the index \n",
    "    tensor along a given axis dim. \n",
    "    '''\n",
    "    \n",
    "    index_size = list(repeat(1, src.dim()))\n",
    "    index_size[dim] = src.size(dim)\n",
    "    index = index.view(index_size).expand_as(src)\n",
    "    \n",
    "    dim = range(src.dim())[dim]\n",
    "    out_size = list(src.size())\n",
    "    out_size[dim] = dim_size\n",
    "\n",
    "    out = src.new_full(out_size, fill_value)\n",
    "\n",
    "    return out.scatter_add_(dim, index, src)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "71a5_Py-E5Gm"
   },
   "source": [
    "Example usage of scatter_add()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "-f_sjOTQAqA0",
    "outputId": "f2282246-bafa-4453-a3dd-ba9379975d07"
   },
   "outputs": [],
   "source": [
    "# Say you have a graph with 4 nodes, and there are an edge list that describes their connectivities.\n",
    "\n",
    "Edge = torch.LongTensor([[0, 0, 1, 3], # index for i \n",
    "                         [1, 2, 2, 0]]) # index for j \n",
    "\n",
    "# It means that the 0th node is connected to 1st node and the 2nd node; the 1st node is connected to the 2nd node. \n",
    "# For now, let us assume the connections are directed, i.e. 0th node is connected the 1st node, but the 1st node is not connected to the 0th node. \n",
    "# We want pass connection messages from the nodes in the first row to the nodes in the second row in Edge.\n",
    "\n",
    "# And for each edge, we have an message we wanto broadcast from i to j.\n",
    "message_i2j = torch.Tensor([1000., 100., 10., 1.])\n",
    "\n",
    "# We can use scatter_add() function to aggregate these pairwise messages onto each node. \n",
    "\n",
    "node_message = scatter_add(src=message_i2j, # message array for all the directed edge \n",
    "            index=Edge[1], # index to all the jth node to which you want to pass your message \n",
    "            dim=0,         # feature dimension you want to sum over \n",
    "            dim_size=4     # there are 4 nodes \n",
    "            ) \n",
    "\n",
    "print(node_message)\n",
    "\n",
    "# Now you can look at your results, you can see the messages are assigned from message_i2j to all the jth nodes you specified\n",
    "\n",
    "# see the graphical representation here: \"https://github.com/vikram-sundar/ML4MolEng_Spring2022/blob/master/psets/ps4/scatter_add_demo.png\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/"
    },
    "id": "WQuj4iV_GVCt",
    "outputId": "fcc1c6aa-54e1-41c1-8491-d2f6b9cf807c"
   },
   "outputs": [],
   "source": [
    "# If you want your graph to be undirected, i.e. the ith node is connected to the jth node and vice versa, you can perfrom the summation in both direction like this: \n",
    "node_message = scatter_add(src=message_i2j, index=Edge[1], dim=0, dim_size=4) +  scatter_add(src=message_i2j, index=Edge[0], dim=0, dim_size=4)\n",
    "\n",
    "print(node_message)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "1XpZgVt0E5Gn"
   },
   "source": [
    "Example usage of torch.split()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "mdkbEciNdVsL"
   },
   "outputs": [],
   "source": [
    "tensor = torch.ones((5, 2))\n",
    "splits_idx = [2, 3] # list of integers \n",
    "print( torch.split(tensor, splits_idx) ) \n",
    "\n",
    "# you have two tensors with size (2,2) and (3,2) respectively \n",
    "for split in torch.split(tensor, splits_idx):\n",
    "    print(split.shape)\n",
    "    \n",
    "# And you can sum the spllited array separately and stack them together \n",
    "print( torch.stack([split.sum(0) for split in torch.split(tensor, splits_idx)], dim=0) )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "FNZxI5T9dVsM"
   },
   "source": [
    "Your GNN class.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8xjflAkwu27Y"
   },
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import ModuleDict\n",
    "\n",
    "class GNN(torch.nn.Module):\n",
    "    '''\n",
    "        A GNN model \n",
    "    '''\n",
    "    def __init__(self, n_convs=3, n_embed=64):\n",
    "        super(GNN, self).__init__()\n",
    "        \n",
    "        self.atom_embed = nn.Embedding(100, n_embed)\n",
    "        # Declare MLPs in a ModuleList\n",
    "        self.convolutions = nn.ModuleList(\n",
    "            [ \n",
    "                ModuleDict({\n",
    "                    'update_mlp': nn.Sequential(nn.Linear(n_embed, n_embed), \n",
    "                                                nn.ReLU(), \n",
    "                                                nn.Linear(n_embed, n_embed)),\n",
    "                    'message_mlp': nn.Sequential(nn.Linear(n_embed, n_embed), \n",
    "                                                 nn.ReLU(), \n",
    "                                                 nn.Linear(n_embed, n_embed)) \n",
    "                })\n",
    "                for _ in range(n_convs)\n",
    "            ]\n",
    "            )\n",
    "        # Declare readout layers\n",
    "        self.readout = nn.Sequential(nn.Linear(n_embed, n_embed), nn.ReLU(), nn.Linear(n_embed, 1))\n",
    "        \n",
    "    def forward(self, AtomicNum, Edge, Natom):\n",
    "        ################ Code #################\n",
    "        \n",
    "        # Parametrize embedding \n",
    "        h = self.atom_embed(AtomicNum) #eqn. 1\n",
    "        \n",
    "        \n",
    "    \n",
    "        \n",
    "        ################ Code #################\n",
    "        return output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "PmypwqSlu27Z"
   },
   "source": [
    "### 1.4 (5 points) Verify that your GNN preserves permutational invariance"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "rV9gSh4TE5Gp"
   },
   "source": [
    "Run this cell as is to show that your GNN respects permutational invariance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "3F7-dv8su27Z"
   },
   "outputs": [],
   "source": [
    "def permute_graph(z, a, perm):\n",
    "    '''\n",
    "        permute the order of nodes in a molecular graph \n",
    "        \n",
    "        Args: \n",
    "            z(np.array): atomic number array\n",
    "            a(np.array): edge index pairs \n",
    "            \n",
    "        Return: \n",
    "            (np.array, np.array): permuted atomic number, and edge list \n",
    "    '''\n",
    "    \n",
    "    z = np.array(z)\n",
    "    perm = np.array(perm)\n",
    "    assert len(perm) == len(z)\n",
    "    \n",
    "    z_perm = z[perm]\n",
    "    a_perm = np.zeros(a.shape).astype(int)\n",
    "    \n",
    "    for i, edge in enumerate(a):\n",
    "        for j in range(len(edge)):\n",
    "            a_perm[i, j] = np.where(perm==edge[j])[0]\n",
    "    return z_perm, a_perm\n",
    "\n",
    "# node input\n",
    "z_orig = np.array([6, 6, 8, 7])\n",
    "# edge input \n",
    "a_orig = np.array([[0, 0, 1, 2, 3, 0], [1, 2, 0, 0, 0, 3]] )\n",
    "\n",
    "permutation = itertools.permutations([0, 1 ,2, 3])\n",
    "device = 'cuda:0'\n",
    "model = GNN(n_convs=4, n_embed=128).to(device)\n",
    "model.eval()\n",
    "\n",
    "for perm in permutation:\n",
    "    z_perm, a_perm = permute_graph(z_orig, a_orig, perm)\n",
    "    \n",
    "    z = torch.LongTensor(z_perm).to(device)\n",
    "    a = torch.LongTensor(a_perm).to(device)\n",
    "    N = [z.shape[0]]\n",
    "\n",
    "    output = model(z, a, N).item()\n",
    "    \n",
    "    print(\"model output: {:.5f} for perumutation: {}\".format(output, perm)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "b5KEswY8u27a"
   },
   "source": [
    "### 1.5  (10 points) Train and test your GNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "hQ0SG56KE5Gq"
   },
   "source": [
    "The optimizer and scheduler setup."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "u8DqTDDbu27a"
   },
   "outputs": [],
   "source": [
    "from torch import optim\n",
    "\n",
    "optimizer = optim.Adam(model.parameters(), lr=1e-3)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=50, verbose=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "4NzsJvkpE5Gr"
   },
   "source": [
    "A combined train/validation loop, with progress bar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "MMgcJzzzu27a"
   },
   "outputs": [],
   "source": [
    "def loop(model, loader, epoch, evaluation=False):\n",
    "    \n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "        mode = \"eval\"\n",
    "    else:\n",
    "        model.train()\n",
    "        mode = 'train'\n",
    "    batch_losses = []\n",
    "    \n",
    "    # Define tqdm progress bar \n",
    "    tqdm_data = tqdm(loader, position=0, leave=True, desc='{} (epoch #{})'.format(mode, epoch))\n",
    "    \n",
    "    for data in tqdm_data:\n",
    "        \n",
    "        AtomicNumber, Edge, Natom, y = data \n",
    "        AtomicNumber = AtomicNumber.to(device)\n",
    "        Edge = Edge.to(device)\n",
    "        y = y.to(device)\n",
    "        pred = model(AtomicNumber, Edge, Natom)\n",
    "        loss = (pred-y).pow(2).mean() # MSE loss\n",
    "        \n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        postfix = ['batch loss={:.3f}'.format(loss.item()) , \n",
    "                   'avg. loss={:.3f}'.format(np.array(batch_losses).mean())]\n",
    "        \n",
    "        tqdm_data.set_postfix_str(' '.join(postfix))\n",
    "    \n",
    "    return np.array(batch_losses).mean()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "7wV57NyPE5Gs"
   },
   "source": [
    "Run this cell to train your model for 500 epochs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "8rjYSawwu27a"
   },
   "outputs": [],
   "source": [
    "for epoch in range(500):    \n",
    "    train_loss = loop(model, train_loader, epoch)\n",
    "    val_loss = loop(model, val_loader, epoch, evaluation=True)\n",
    "    \n",
    "    # save model \n",
    "    if epoch % 20 == 0:\n",
    "        torch.save(model.state_dict(), \"{}/gcn_model_{}.pt\".format(mydrive, epoch))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "OEPUcAOndePL"
   },
   "source": [
    "Show us a scatter plot of the training and test data, with MSEs labeled."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "83BQu_CeE5Gt"
   },
   "outputs": [],
   "source": [
    "################ Code #################"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "u9wQfnRCLDDS"
   },
   "source": [
    "## Part 2: Variational auto-encoders for SMILES strings"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "gf8x8c43LDDU"
   },
   "outputs": [],
   "source": [
    "# Get data \n",
    "! wget https://raw.githubusercontent.com/YitongTseo/ML4MolEng_Spring2023/master/psets/ps5/data/zinc_50k.csv\n",
    "    \n",
    "# Get pretrained model\n",
    "! wget -O vae_checkpoint.pth https://raw.githubusercontent.com/YitongTseo/ML4MolEng_Spring2023/master/psets/ps5/pretrained_checkpoints/vae-050-0.06.pth"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "355HMSjvLDDV"
   },
   "source": [
    "### 2.1 (5 points) One-hot encode SMILES strings into padded numerical vectors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "HhdMBRF3LDDV"
   },
   "outputs": [],
   "source": [
    "from sklearn import preprocessing\n",
    "\n",
    "# Character list for SMILES string\n",
    "moses_charset = ['2', 'o', 'C', 'I', 'O', 'H', 'n', 'N', '=', '+', '#', '-', 'c',\n",
    "                 'B', 'l', '7', 'r', 'S', 's', '4', '6', '[', '5', ']', 'F', '3', \n",
    "                 'P', '(', ')', '1', ' ']\n",
    "\n",
    "# Define encoder \n",
    "enc = preprocessing.LabelEncoder().fit(moses_charset)\n",
    "\n",
    "# Read data \n",
    "df = pd.read_csv(\"./zinc_50k.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Fumu4UArLDDV"
   },
   "source": [
    "Encode SMILES strings into padded categorical vectors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "pMF6gbLVLDDW"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "39Na-oiOLDDW"
   },
   "source": [
    "Make train/validation/test Datasets and DataLoaders."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "AP580LJHLDDW"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "8UQEv61MLDDW"
   },
   "source": [
    "### 2.2 (14 points) Implement the Reparametrization Trick for VAE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "Ol5NTLLELDDW"
   },
   "outputs": [],
   "source": [
    "# Molecular VAE model \n",
    "\n",
    "class MolVAE(nn.Module):\n",
    "    def __init__(self,  rnn_enc_hid_dim, enc_nconv,\n",
    "                         encoder_hid, z_dim, \n",
    "                         rnn_dec_hid_dim, dec_nconv, smiles_len, nchar\n",
    "                         ):\n",
    "        '''\n",
    "            SMILES VAE model \n",
    "            \n",
    "                rnn_enc_hid_dim: hidden dimension for the GRU encoder \n",
    "                enc_nconv: number of recurrent layers for the GRU decoder\n",
    "                encoder_hid: dimension of GUR encoder readout\n",
    "                z_dim: number of latent variable \n",
    "                rnn_dec_hid_dim: hidden dimension for the GRU decoder \n",
    "                dec_nconv: number of recurrent layers for the GRU decoder\n",
    "                smiles_len: total length of padded SMILES string \n",
    "                nchar: number of possible characters \n",
    "                \n",
    "        '''\n",
    "        \n",
    "        super(MolVAE, self).__init__()\n",
    "        \n",
    "        self.smiles_len = smiles_len\n",
    "        self.nchar = nchar\n",
    "        # Embedding layer\n",
    "        self.embed = nn.Embedding(self.nchar, rnn_enc_hid_dim)\n",
    "        # Encoding GRU\n",
    "        self.rnn_enc = nn.GRU(rnn_enc_hid_dim, rnn_enc_hid_dim, enc_nconv, batch_first=True)\n",
    "        # MLP to transfrom hidden output from Encoding GRU\n",
    "        self.mlp0 = nn.Linear(rnn_enc_hid_dim, encoder_hid)\n",
    "        # Network to parametrize mu\n",
    "        self.mu_network = nn.Linear(encoder_hid, z_dim)\n",
    "        # Network to parametrize log variance\n",
    "        self.logvar_network = nn.Linear(encoder_hid, z_dim)\n",
    "        # Decoding GRU\n",
    "        self.rnn_dec = nn.GRU(z_dim, rnn_dec_hid_dim, dec_nconv, batch_first=True)\n",
    "        # Output SMILES characters\n",
    "        self.readout = nn.Linear(rnn_dec_hid_dim, self.nchar)\n",
    "\n",
    "    def encode(self, x):\n",
    "        '''Output mean and log variance of the encoded SMILES'''\n",
    "        output, hn = self.rnn_enc(x)\n",
    "        h = torch.nn.functional.relu(self.mlp0(hn[-1]))\n",
    "        return self.mu_network(h), self.logvar_network(h)\n",
    "    \n",
    "    def get_std(self, logvar):\n",
    "        '''Transform log variance to standard deviation'''\n",
    "        ################ Code #################\n",
    "        \n",
    "        return logvar\n",
    "        ################ Code #################\n",
    "\n",
    "    def reparametrize(self, mu, std):\n",
    "        '''The reparametrization trick'''\n",
    "        if self.training:\n",
    "            ################ Code #################\n",
    "            \n",
    "            \n",
    "            return z\n",
    "           ################ Code ################# \n",
    "        else:\n",
    "            return mu\n",
    "\n",
    "    def decode(self, z):\n",
    "        '''Decoder to reconstruct latent variable back to SMILES'''\n",
    "        z = z.view(z.size(0), 1, z.size(-1)).repeat(1, self.smiles_len, 1)\n",
    "        out, h = self.rnn_dec(z)\n",
    "        out_reshape = out.contiguous().view(-1, out.size(-1))\n",
    "        \n",
    "        y0 = self.readout(out_reshape)\n",
    "        y = y0.contiguous().view(out.size(0), -1, y0.size(-1))\n",
    "        return y\n",
    "\n",
    "    def forward(self, x):\n",
    "        x_embed = self.embed(x) # Get SMILES embedding \n",
    "        mu, logvar = self.encode(x_embed) # Encoding SMILES to latent representations \n",
    "        std = self.get_std(logvar) # Transform log variance to std.\n",
    "        z = self.reparametrize(mu, std) # Reparametrization trick \n",
    "        smiles_recon = self.decode(z)  # Reconstruct SMILES string \n",
    "        return smiles_recon, mu, std"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "qwyf09mALxZm"
   },
   "source": [
    "Test your model by comparing your sampling with N(0, 1)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "_lP9coMuLDDY"
   },
   "outputs": [],
   "source": [
    "# Define your model \n",
    "model = MolVAE(rnn_enc_hid_dim=256, enc_nconv=1, \n",
    "                     encoder_hid=256, z_dim=128, rnn_dec_hid_dim=512,\n",
    "                    dec_nconv=3, nchar=31, smiles_len=max_len)\n",
    "\n",
    "# Compare your sampling with N(0, 1)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import norm\n",
    "\n",
    "sample = model.reparametrize(torch.zeros(1000), torch.ones(1000))\n",
    "plt.hist(sample.detach().cpu().numpy(), density=True)\n",
    "\n",
    "# Plot between -10 and 10 with .001 steps.\n",
    "x_axis = np.arange(-7, 7, 0.001)\n",
    "plt.plot(x_axis, norm.pdf(x_axis,0,1)) # Mean = 0, SD = 1.\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0WLmVkJKLDDY"
   },
   "source": [
    "### 2.3 (14 points) Implement the SMILES VAE loss function"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "Ka40eSCOE5Gx"
   },
   "source": [
    "Implement your loss function here."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IgkqLI_jLDDY"
   },
   "outputs": [],
   "source": [
    "def loss_function(recon_x, x, mu, std):\n",
    "    ################ Code #################\n",
    "    L_recon = 0.0  # reconstruction loss \n",
    "    L_kl = 0.0 # KL loss\n",
    "    return L_recon, L_kl"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "0IrBh0R2E5Gx"
   },
   "source": [
    "### 2.4 (2 points) Train your model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "abipdkH0E5Gx"
   },
   "source": [
    "Run the following cells to train your model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "W9Svto16LDDZ"
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def loop(model, loader, epoch, beta=0.05, evaluation=False):\n",
    "    '''\n",
    "        Train/test your VAE model\n",
    "    '''\n",
    "    \n",
    "    if evaluation:\n",
    "        model.eval()\n",
    "        mode = \"eval\"\n",
    "    else:\n",
    "        model.train()\n",
    "        mode = 'train'\n",
    "    batch_losses = []\n",
    "        \n",
    "    tqdm_data = tqdm(loader, position=0, leave=True, desc='{} (epoch #{})'.format(mode, epoch))\n",
    "    for data in tqdm_data:\n",
    "        \n",
    "        x = data[0].to(device)\n",
    "        recon_batch, mu, std = model(x)\n",
    "        loss_recon, loss_kl = loss_function(recon_batch, x, mu, std)\n",
    "        loss = loss_recon + beta * loss_kl     \n",
    "        \n",
    "        if not evaluation:\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "        batch_losses.append(loss.item())\n",
    "\n",
    "        postfix = ['recon loss={:.3f}'.format(loss_recon.item()) ,\n",
    "                   'KL loss={:.3f}'.format(loss_kl.item()) ,\n",
    "                   'total loss={:.3f}'.format(loss.item()) , \n",
    "                   'avg. loss={:.3f}'.format(np.array(batch_losses).mean())]\n",
    "        \n",
    "        tqdm_data.set_postfix_str(' '.join(postfix))\n",
    "    \n",
    "    return np.array(batch_losses).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dQ6CG8DiLDDa"
   },
   "outputs": [],
   "source": [
    "device = 0\n",
    "\n",
    "model = MolVAE(rnn_enc_hid_dim=367, enc_nconv=2, \n",
    "                     encoder_hid=512, z_dim=171, rnn_dec_hid_dim=512,\n",
    "                    dec_nconv=1, nchar=31, smiles_len=max_len)\n",
    "                      \n",
    "model = model.to(device)\n",
    "\n",
    "# load pretrained model \n",
    "model.load_state_dict(torch.load(\"./vae_checkpoint.pth\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "BgHPDig-LDDa"
   },
   "outputs": [],
   "source": [
    "optimizer = optim.Adam(model.parameters(),lr=5e-5)\n",
    "scheduler = optim.lr_scheduler.ReduceLROnPlateau(optimizer, 'min', factor=0.5, patience=5, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "IayKlIjoLDDa"
   },
   "outputs": [],
   "source": [
    "epochs = 50\n",
    "\n",
    "for epoch in range(0, epochs):\n",
    "    \n",
    "    train_loss = loop(model, train_loader, epoch, 0.001)\n",
    "    val_loss = loop(model, val_loader, epoch, 0.001,  evaluation=True)\n",
    "    scheduler.step(val_loss)\n",
    "    \n",
    "    # optional: save model \n",
    "#     if epoch % 15 == 0:\n",
    "#         torch.save(model.state_dict(),\n",
    "#                 './{}/vae-{:03d}-{:.2f}.pth'.format(mydrive, epoch, train_loss))\n",
    "        \n",
    "#         torch.save(optimizer.state_dict(),\n",
    "#             './{}/optim-{:03d}-{:.2f}.pth'.format(mydrive, epoch, train_loss))\n",
    "\n",
    "    if epoch == 0:\n",
    "        best_loss = train_loss.item()\n",
    "    else:\n",
    "        if train_loss.item() < best_loss:\n",
    "            best_loss = train_loss.item()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "irPJVr5GLDDa"
   },
   "source": [
    "### 2.5 (10 points) Sample new molecules"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "k9ehHg6nE5G0"
   },
   "source": [
    "Some helper functions for you."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "qJ7vs1URE5G1"
   },
   "outputs": [],
   "source": [
    "def index2smiles(mol_index, enc):\n",
    "    '''Transform your array of character indices back to SMILES'''\n",
    "    smiles_charlist = enc.inverse_transform(np.array(mol_index))\n",
    "    smiles = ''.join(smiles_charlist).strip(\" \")\n",
    "    \n",
    "    return smiles\n",
    "\n",
    "def check_smiles_valid(smiles):\n",
    "    '''Check if SMILES string is valid'''\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol:\n",
    "        valid = True \n",
    "    else:\n",
    "        valid = False \n",
    "    return valid"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "IlZmTzzUE5G1"
   },
   "source": [
    "Randomly select two SMILES in your test data, interpolate 10 points between them, and decode those points. Test them for accuracy and draw the scatter plot of the lower 2 dimensions. Then visualize any molecules that worked."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "dhHlUOxQLDDb"
   },
   "outputs": [],
   "source": [
    "################ Code #################\n",
    "# select a starting and ending molecule\n",
    "\n",
    "start = index2smiles(test_loader.dataset.__getitem__(random.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "end = index2smiles(test_loader.dataset.__getitem__(random.choices(range(len(test_loader.dataset)), k=1))[0].numpy().reshape(-1), enc)\n",
    "\n",
    "model.eval()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "id": "RQAB517EfdSR"
   },
   "source": [
    "Why does the VAE sometimes fail to generate valid SMILES strings?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "id": "ZNEpmwLNdXjq"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "name": "ps4_template.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
