{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#  <center> Problem Set 1 <center>\n",
    "\n",
    "<center> 3.C01/3.C51, 7.C01/7.C51, 10.C01/10.C51, 20.C01/20.C51 <center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<b>Name:</b>\n",
    "\n",
    "<b>Kerberos ID:</b>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Instructions: \n",
    "\n",
    "Put your code in the code block like this:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### ùô≤ùöòùöçùöé #############  \n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "\n",
    "def mycode():\n",
    "    print(\"Run my code here\")\n",
    "    \n",
    "def plot():\n",
    "    plt.hist(np.ones(100))\n",
    "\n",
    "mycode()\n",
    "plot()\n",
    "########### ùô≤ùöòùöçùöé ############# "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided print statements where numerical answers are expected. \n",
    "\n",
    "Your answer should be contained in a variable which you defined either in the Answer Block or the Code Block.\n",
    "\n",
    "When a qualitative answer is expected, you can write answer as code comments by placing a # before your answer.\n",
    "\n",
    "Your Answer Block should look like the following: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############ + $\n",
    "\n",
    "ans = 2\n",
    "print(\"My answer is: {}.\".format(ans))\n",
    "\n",
    "# My regressor over-fitted the training data, I need to add regularization + $\n",
    "\n",
    "########## Answer ############ + $"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import packages\n",
    "import numpy as np\n",
    "import sklearn\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# models \n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "# metrics \n",
    "from sklearn.metrics import ConfusionMatrixDisplay, RocCurveDisplay, roc_auc_score\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.pipeline import Pipeline\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# plotting style, you can choose your own parameters\n",
    "import matplotlib\n",
    "\n",
    "matplotlib.rcParams.update({'font.size': 15})\n",
    "matplotlib.rc('lines', linewidth=3, color='g')\n",
    "matplotlib.rcParams['axes.linewidth'] = 2.0\n",
    "matplotlib.rcParams['axes.linewidth'] = 2.0\n",
    "matplotlib.rcParams[\"xtick.major.size\"] = 6\n",
    "matplotlib.rcParams[\"ytick.major.size\"] = 6\n",
    "matplotlib.rcParams[\"ytick.major.width\"] = 2\n",
    "matplotlib.rcParams[\"xtick.major.width\"] = 2\n",
    "matplotlib.rcParams['text.usetex'] = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# A helper function for students to produce plots \n",
    "def plot_clf(model, X, y, title): \n",
    "    \n",
    "    '''\n",
    "        A function to plot confusion matrix and ROC curve \n",
    "        \n",
    "        Args: \n",
    "            model(classifier object): model object (e.g. RandomForestClassifier, LogisticRegression)\n",
    "            X(np.array): feature set\n",
    "            y(np.array): label set \n",
    "            title(str): plot name\n",
    "            \n",
    "        Example Usage: \n",
    "            plot_clf(model, X_test, y_test, \"test\")\n",
    "    '''\n",
    "    \n",
    "    fig, [ax_roc, ax_conf] = plt.subplots(1, 2, figsize=(12, 6))\n",
    "    fig.tight_layout()\n",
    "    \n",
    "    RocCurveDisplay.from_estimator(model, X, y, ax=ax_roc)\n",
    "    ConfusionMatrixDisplay.from_estimator(model, X, y, ax=ax_conf)\n",
    "\n",
    "    ax_roc.set_title('{} ROC'.format(title))\n",
    "    ax_conf.set_title('{} Confusion Matrix'.format(title))\n",
    "\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <center>Problem 1 (Preliminary Modeling) <center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Download data \n",
    "\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/master/psets/ps1/data/breastcancer_X.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/master/psets/ps1/data/breastcancer_y.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.1 (5 points) Load and inspect the raw data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We have provided the code to load the dataset. Take a moment to understand what each line is doing. Briefy explain what each line of the code is doing by providing short comments below. \n",
    "\n",
    "You will have to do it by yourself again in Problem 2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p1_X = pd.read_csv(\"./breastcancer_X.csv\", header='infer', index_col=0) # Comments here \n",
    "p1_y = pd.read_csv(\"./breastcancer_y.csv\", header='infer', index_col=0) # Comments here \n",
    "\n",
    "metabolite_name = p1_X.columns.tolist() # Comments here \n",
    "\n",
    "p1_X = p1_X.values # Comments here \n",
    "p1_y = p1_y.values # Comments here "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report how many examples are in this dataset and the number of features for each data point."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"There are {} samples.\".format(N_samples))\n",
    "print(\"There are {} features per sample.\".format(N_features))\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.2 (5 points) Generate train/test splits"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split the dataset into train and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the shapes of your four variables, X_train, X_test, y_train, and y_test, and ensure sure that the dimensions match your expectations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "print(\"X_train shape: {}\".format(X_train_shape))\n",
    "print(\"y_train shape: {}\".format(y_train_shape))\n",
    "\n",
    "print(\"X_test shape: {}\".format(X_test_shape))\n",
    "print(\"y_test shape: {}\".format(y_test_shape))\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.3 (5 points) Preprocess the data through scaling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Scale the dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Print the mean/variance for each transformed feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\n",
    "    \"The average means of the transformed feature train set are {}\".format(\n",
    "        np.mean(X_train_scaled)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The average variances of the transformed feature train set are {}\".format(\n",
    "        np.std(X_train_scaled)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The average mean of the transformed feature test set are {}\".format(\n",
    "        np.mean(X_test_scaled)\n",
    "    )\n",
    ")\n",
    "print(\n",
    "    \"The average variances of the transformed feature test set are {}\".format(\n",
    "        np.std(X_test_scaled)\n",
    "    )\n",
    ")\n",
    "\n",
    "# Discuss the importance of not fitting the scaler transform on both train & test, and why the transformed mean / variance \n",
    "# between X_test_scaled & X_train_scaled may not be the same and why the difference is potentially heartening.\n",
    "\n",
    "# Comment here: \n",
    "\n",
    "########## Answer ############\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.4 (15 points) Train and evaluate a logistic regression model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train and evaluate a Logistic Regression model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the AUC score for both training and testing data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "print(\"The training AUC score is {:.3f}\".format(train_auc) )\n",
    "print(\"The testing AUC score is {:.3f}\".format(test_auc) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate plots for the confusion matrices and the ROC curve for both training and testing. Please use the plot_clf function defined above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.5 (5 points) Perform a more thorough cross-validation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perform cross-validation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "scaler = # fill this in (reusing your scaler implementation from 1.3)\n",
    "model = # fill this in (reusing your model declaration in 1.4)\n",
    "pipeline = Pipeline([('scaler', scaler), ('model', model)])\n",
    "\n",
    "# Now call cross_val_score() and feed it: your pipeline as the estimator \n",
    "# and your data (p1_X & p1_y) letting cross_val_score() handle the test / train split for you.\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the cross validated ROC-AUC score in terms of its mean and standard deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The mean of CV scores is {:.2f}\".format(np.mean(scores)) )\n",
    "print(\"The std of CV scores is {:.2f}\".format(np.std(scores)) )\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.6  (optional +2.5 points) Connect model coefficients back to metabolites"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identify the top 5 metabolites that positively correlated the most with positive diagnosis."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the metabolites you identified."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The top 5 metabolites are {}\".format(\", \".join(metabolites)) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1.7 (Grad 10 points) Quantify the effects of a reduced feature space"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For each hypothetical featurization, obtain an average ROC-AUC and its standard deviation through a 5-fold CV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use matplotlib to generate one scatter plot showing these cross-validated test AUC score (y-axis) as a function of the different feature subset sizes (x-axis) with error bars corresponding to the standard deviations from different subset sampling. Here, we provide an example plotting code. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "# generate random data \n",
    "feature_sizes = [0.05, 0.1, 0.2, 0.4, 0.6, 0.8]\n",
    "y_auc = np.random.randn(6)\n",
    "y_uncer = np.random.uniform(0.2, 1, size=6) * 0.25\n",
    "\n",
    "# plotting code \n",
    "plt.figure(figsize=(5,5)) # define figure size \n",
    "plt.errorbar(feature_sizes, y_auc, y_uncer, marker='o', ms=10, mew=2, label='line 1') \n",
    "plt.xlabel('feature sizes')\n",
    "plt.ylabel('CV-AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Problem 2 (Adding regularization to reduce overfitting)</center>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/master/psets/ps1/data/liver_X.csv\n",
    "!wget https://raw.githubusercontent.com/coleygroup/ML4MolEng/master/psets/ps1/data/liver_y.csv"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.1 (10 points) Unregularized logistic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Follow the same procedure from Part 1 to load the data, split the data, and scale the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Train a logistic regression model, and test its performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Confusion Matrix and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report your test ROC-AUC score. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The training AUC score is {:.2f}\".format(train_auc) )\n",
    "print(\"The testing AUC score is {:.2f}\".format(test_auc) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.2 (5 points) Introduce L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run L1-regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report ROC-AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The training AUC score is {:.2f}\".format(train_auc) )\n",
    "print(\"The testing AUC score is {:.2f}\".format(test_auc) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Confusion Matrix and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the histogram of model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on the histogram you obtained. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.3 (5 points) Introduce L2 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run L2-regularized logistic regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report test ROC-AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The training AUC score is {:.2f}\".format(train_auc) )\n",
    "print(\"The testing AUC score is {:.2f}\".format(test_auc) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate Confusion Matrix and ROC curve."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Generate the histogram of model coefficients."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment on your histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2.4 (Grad 10 points) Quantify the extent of overfitting as a function of training set size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run un-regularized and L1-regularized logistic regressions for the following random train/test splits:\n",
    "\n",
    "{20%/30%, 30%/30%, 40%/30%, 50%/30%, 60%/30%}. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Plot training/testing ROC-AUC score as a function of train size for L1-regularized and un-regularized regressions. We have provided example plotting code with random data. You can use it as a template to generate two plots: one for the L1-regularized regression, and one for the un-regularized regression."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "# generate random data \n",
    "train_sizes = [0.2, 0.3, 0.4, 0.5, 0.6]\n",
    "y_auc = np.random.randn(5)\n",
    "y_uncer = np.random.uniform(0.2, 1, size=5)\n",
    "\n",
    "# Define plots \n",
    "plt.figure(figsize=(5,5))\n",
    "\n",
    "# first plot\n",
    "plt.errorbar(train_sizes, y_auc, y_uncer,\n",
    "             marker='o', color='blue', linestyle='--',\n",
    "             ms=10, mew=2, label='L1 train')\n",
    "\n",
    "# 2nd plot\n",
    "plt.errorbar(train_sizes, y_auc + 2, y_uncer, \n",
    "             marker='o', color='orange',  ms=10, linestyle='-', \n",
    "             mew=2, label='L1 test') # plotting for test score\n",
    "\n",
    "plt.xlabel('train sizes')\n",
    "plt.ylabel('AUC')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Comment  on  when  the  model  is  most  prone  to  poor generalization (i.e., a large gap between training and testing performance):  with or without regularization?  For large or small training set sizes?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <center>Problem 3: Random Forests and Hyperparameter Optimization</center>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 (5 points) Train a Random Forest classifier"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run cross-validation to train a Random Forest classifier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the cross-validated ROC-AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The mean of CV scores is {:.2f}\".format(mean) )\n",
    "print(\"The std of CV scores is {:.2f}\".format(std) )\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.2 (15 points) Perform a hyperparameter optimization with a grid search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "parameters = {'model__n_estimators': [20, 40, 80, 160, 320, 640, 1280], \n",
    "              'model__min_samples_split': [8, 10, 12, 24], \n",
    "              'model__max_depth': [2, 4, 8]}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Describe the total number of possible parameter combinations given the parameter ranges given."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"There are {} different combinations\".format(N_comb))\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Use the provided parameter set to run grid hyperparameter search with cross-validation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########### Code #############\n",
    "\n",
    "\n",
    "\n",
    "########### Code #############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the obtained parameter set. Answer in code comments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "# The optimized parameter set is:\n",
    "# {\n",
    "#    'model__n_estimators': #answer,  \n",
    "#    'model__min_samples_split': #answer,\n",
    "#    'model__max_depth': #answer\n",
    "# }\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Report the best cross-validation score from your grid search."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The best cross-validated AUC score is {:.2f}\".format(clf.best_score_))\n",
    "\n",
    "########## Answer ############"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make predictions on the test data with your best model and report the AUC score."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "########## Answer ############\n",
    "\n",
    "print(\"The test AUC score calculated with the best model is {:.2f}\".format(test_auc))\n",
    "\n",
    "########## Answer ############"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "aee8b7b246df8f9039afb4144a1f6fd8d2ca17a180786b69acc140d282b71a49"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
